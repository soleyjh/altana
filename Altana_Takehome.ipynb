{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692052c2",
   "metadata": {},
   "source": [
    "## Altana A.I. Takehome Assignment \n",
    "### Candidate for Hire: James (Jay) Soley\n",
    "#### July 29, 2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8ae9d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (2,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Import Numpy and Pandas Libraries and Load in CSV Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "train_df = pd.read_csv('/Users/jamessoley/Documents/altana/ds-project-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3886d1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>WEIGHT..LB.</th>\n",
       "      <th>WEIGHT..KG.</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>MEASUREMENT</th>\n",
       "      <th>CONTAINER.COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>143280.000000</td>\n",
       "      <td>1.316930e+05</td>\n",
       "      <td>1.316930e+05</td>\n",
       "      <td>1.316930e+05</td>\n",
       "      <td>131693.000000</td>\n",
       "      <td>131693.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>95604.264391</td>\n",
       "      <td>3.868200e+04</td>\n",
       "      <td>1.758005e+04</td>\n",
       "      <td>1.519561e+03</td>\n",
       "      <td>79.271412</td>\n",
       "      <td>1.564381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>55174.831229</td>\n",
       "      <td>1.155783e+06</td>\n",
       "      <td>5.251678e+05</td>\n",
       "      <td>7.820183e+04</td>\n",
       "      <td>1409.380657</td>\n",
       "      <td>2.070290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47799.750000</td>\n",
       "      <td>1.357400e+03</td>\n",
       "      <td>6.170000e+02</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>95690.500000</td>\n",
       "      <td>6.010400e+03</td>\n",
       "      <td>2.732000e+03</td>\n",
       "      <td>4.650000e+02</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>143374.500000</td>\n",
       "      <td>2.398600e+04</td>\n",
       "      <td>1.090270e+04</td>\n",
       "      <td>1.200000e+03</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>191040.000000</td>\n",
       "      <td>1.331000e+08</td>\n",
       "      <td>6.050000e+07</td>\n",
       "      <td>2.833283e+07</td>\n",
       "      <td>431431.000000</td>\n",
       "      <td>127.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0   WEIGHT..LB.   WEIGHT..KG.      QUANTITY    MEASUREMENT  \\\n",
       "count  143280.000000  1.316930e+05  1.316930e+05  1.316930e+05  131693.000000   \n",
       "mean    95604.264391  3.868200e+04  1.758005e+04  1.519561e+03      79.271412   \n",
       "std     55174.831229  1.155783e+06  5.251678e+05  7.820183e+04    1409.380657   \n",
       "min         1.000000  0.000000e+00  0.000000e+00  1.000000e+00       0.000000   \n",
       "25%     47799.750000  1.357400e+03  6.170000e+02  1.460000e+02       1.000000   \n",
       "50%     95690.500000  6.010400e+03  2.732000e+03  4.650000e+02      10.000000   \n",
       "75%    143374.500000  2.398600e+04  1.090270e+04  1.200000e+03      48.000000   \n",
       "max    191040.000000  1.331000e+08  6.050000e+07  2.833283e+07  431431.000000   \n",
       "\n",
       "       CONTAINER.COUNT  \n",
       "count    131693.000000  \n",
       "mean          1.564381  \n",
       "std           2.070290  \n",
       "min           0.000000  \n",
       "25%           1.000000  \n",
       "50%           1.000000  \n",
       "75%           1.000000  \n",
       "max         127.000000  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3533b3",
   "metadata": {},
   "source": [
    "### Investigate the Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc0624a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY.OF.ORIGIN</th>\n",
       "      <th>country_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>94041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>4934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>4271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>4147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oman</td>\n",
       "      <td>2345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Panama</td>\n",
       "      <td>2298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>China Taiwan</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>2061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Guatemala</td>\n",
       "      <td>1659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Spain</td>\n",
       "      <td>1374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>1272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>India</td>\n",
       "      <td>1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Japan</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chile</td>\n",
       "      <td>776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Germany</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Italy</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bahamas</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Honduras</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Jamaica</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Canada</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Dominican Republic</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Colombia</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Haiti</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>France</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Israel</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Romania</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Greece</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Jordan</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>United Arab Em</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Australia</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Peru</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Trinidad</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Morocco</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Bermuda</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Ecuador</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Malta</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Cambodia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Neth Antilles</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Taiwan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>US Virgin Is</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Federal Republic of Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Belize</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Guam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Cayman Isl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Kuwait</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Guadeloupe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              COUNTRY.OF.ORIGIN  country_count\n",
       "0                         China          94041\n",
       "1                     Hong Kong           4934\n",
       "2                   South Korea           4271\n",
       "3                     Singapore           4147\n",
       "4                          Oman           2345\n",
       "5                        Panama           2298\n",
       "6                  China Taiwan           2192\n",
       "7                      Malaysia           2061\n",
       "8                     Guatemala           1659\n",
       "9                         Spain           1374\n",
       "10                   Costa Rica           1272\n",
       "11                        India           1202\n",
       "12                        Japan            835\n",
       "13                        Chile            776\n",
       "14                    Sri Lanka            743\n",
       "15                     Pakistan            670\n",
       "16                      Germany            643\n",
       "17                     Thailand            616\n",
       "18                      Vietnam            527\n",
       "19                       Mexico            389\n",
       "20                        Italy            354\n",
       "21                       Brazil            289\n",
       "22                        Egypt            287\n",
       "23               American Samoa            252\n",
       "24                      Bahamas            250\n",
       "25                     Honduras            248\n",
       "26                      Belgium            208\n",
       "27                      Jamaica            190\n",
       "28               United Kingdom            188\n",
       "29                       Sweden            150\n",
       "30                       Canada            128\n",
       "31           Dominican Republic            122\n",
       "32                  Netherlands            106\n",
       "33                     Colombia            104\n",
       "34                        Haiti             83\n",
       "35                       France             62\n",
       "36                 South Africa             59\n",
       "37                       Israel             55\n",
       "38                      Romania             54\n",
       "39                       Greece             46\n",
       "40                     Portugal             43\n",
       "41                    Argentina             34\n",
       "42                       Jordan             31\n",
       "43                       Turkey             22\n",
       "44               United Arab Em             15\n",
       "45                      Unknown             15\n",
       "46                    Australia             11\n",
       "47                         Peru             10\n",
       "48                     Trinidad              9\n",
       "49                      Morocco              8\n",
       "50                    Indonesia              7\n",
       "51                      Bermuda              7\n",
       "52                  New Zealand              6\n",
       "53                      Ecuador              6\n",
       "54                      Iceland              5\n",
       "55                        Malta              4\n",
       "56                    Venezuela              4\n",
       "57                 Saudi Arabia              3\n",
       "58                     Cambodia              2\n",
       "59                Neth Antilles              2\n",
       "60                       Taiwan              2\n",
       "61                 US Virgin Is              1\n",
       "62  Federal Republic of Germany              1\n",
       "63                  Philippines              1\n",
       "64                       Belize              1\n",
       "65                         Guam              1\n",
       "66                   Cayman Isl              1\n",
       "67                      Denmark              1\n",
       "68                       Kuwait              1\n",
       "69                   Guadeloupe              1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pivot = train_df.groupby('COUNTRY.OF.ORIGIN')['COUNTRY.OF.ORIGIN'].count().sort_values(ascending=False).to_frame('country_count').reset_index()\n",
    "train_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec6091",
   "metadata": {},
   "source": [
    "#### Clearly we see the the balance of the classes is very imbalanced with China representing ~ 70% of the overall data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32931f",
   "metadata": {},
   "source": [
    "#### Investigate if Labels and Features contain missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f155460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COUNTRY.OF.ORIGIN    12795\n",
       "ARRIVAL.DATE         11587\n",
       "WEIGHT..KG.          11587\n",
       "US.PORT              12093\n",
       "PRODUCT.DETAILS      11662\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Only select the Labels and Features\n",
    "train_df[['COUNTRY.OF.ORIGIN','ARRIVAL.DATE','WEIGHT..KG.','US.PORT','PRODUCT.DETAILS']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5551e",
   "metadata": {},
   "source": [
    "#### We're missing 12.7K Class Labels according to above -- let's take a look at what some of these rows look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f7494251",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's Take a Look at the Dataset so we can better see what's actually happening with the NULL Values\n",
    "bool_series = pd.isnull(train_df['COUNTRY.OF.ORIGIN'])\n",
    "#train_df[bool_series]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f94a2",
   "metadata": {},
   "source": [
    "#### It's unclear to me why we have data with missing class labels but I don't think it makes very much sense to train on this data and I don't see any pattern which can allow me to impute the class labels from domain knowledge of the dataset. For example, I thought \"Carrier.City\" might be related to Country.Of.Origin but it doesn't appear that way. Therefore, I think the best answer is remove observations where we don't have the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e55a54cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 0\n",
       "SHIPPER                 9278\n",
       "SHIPPER.ADDRESS        99714\n",
       "CONSIGNEE                  0\n",
       "CONSIGNEE.ADDRESS      97922\n",
       "ZIPCODE                10613\n",
       "NOTIFY                 14586\n",
       "NOTIFY.ADDRESS         17043\n",
       "BILL.OF.LADING             0\n",
       "ARRIVAL.DATE               0\n",
       "WEIGHT..LB.                0\n",
       "WEIGHT..KG.                0\n",
       "US.PORT                    0\n",
       "QUANTITY                   0\n",
       "Q.UNIT                   768\n",
       "MEASUREMENT                0\n",
       "M.UNIT                 25229\n",
       "SHIP.REGISTERED.IN         8\n",
       "VESSEL.NAME                0\n",
       "CONTAINER.COUNT            0\n",
       "PRODUCT.DETAILS           75\n",
       "MARKS.AND.NUMBERS       1205\n",
       "COUNTRY.OF.ORIGIN          0\n",
       "DISTRIBUTION.PORT     101597\n",
       "CARRIER.CODE               0\n",
       "CARRIER.NAME             296\n",
       "CARRIER.ADDRESS         2604\n",
       "CARRIER.CITY            5603\n",
       "CARRIER.STATE          60805\n",
       "CARRIER.ZIP            28464\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Remove Rows with missing class label from the dataset\n",
    "train_df_labels = train_df[bool_series == False]\n",
    "train_df_labels.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1416ead",
   "metadata": {},
   "source": [
    "#### It appears that by removing the rows with the missing class labels that we've also solved our issues with the missing values in the features that we're interested in using to predict the Country.Of.Origin: US.Port, WEIGHT..KG., ARRIVAL.DATE, PRODUCT.DETAILS (is missing a small amount so nbd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa47910",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "#### Let's take a closer look at how some of the features look so we can devise a strategy of attack\n",
    "##### Plan: B/c I'm heading towards XGBoost to fit my classifier (a tree based model) I don't plan to change the Weight.KG variable as it doesn't need feature engineering. I do plan on turning port into categoricals using one hot encoding and the temporal feature also needs to be coded in a way that XGBoost can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3cf1871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   Long Beach, California\n",
      "6                                    Jacksonville, Florida\n",
      "7                                        Norfolk, Virginia\n",
      "47                                       Savannah, Georgia\n",
      "53                              Charleston, South Carolina\n",
      "64                                     Seattle, Washington\n",
      "169                                Los Angeles, California\n",
      "412               New York/Newark Area, Newark, New Jersey\n",
      "413                                         Houston, Texas\n",
      "1105                                       Mobile, Alabama\n",
      "1158                                    Tacoma, Washington\n",
      "1468                            Philadelphia, Pennsylvania\n",
      "1970                                 San Juan, Puerto Rico\n",
      "2162                                    New York, New York\n",
      "5175                                New Orleans, Louisiana\n",
      "6224                                   Oakland, California\n",
      "9278                              Port Everglades, Florida\n",
      "9499                                  Wilmington, Delaware\n",
      "9607                                        Miami, Florida\n",
      "10967                           Wilmington, North Carolina\n",
      "13232                                  Baltimore, Maryland\n",
      "13913                                       Tampa, Florida\n",
      "14056                                     Portland, Oregon\n",
      "15388                            San Francisco, California\n",
      "16281                              Port Canaveral, Florida\n",
      "17837                                     Honolulu, Hawaii\n",
      "20930                                Boston, Massachusetts\n",
      "22979                                Vancouver, Washington\n",
      "23135                                             NEW YORK\n",
      "26187                             West Palm Beach, Florida\n",
      "26209                                         VANCOUVER BC\n",
      "26280                                San Diego, California\n",
      "26955                                              OAKLAND\n",
      "27645                            Fernandina Beach, Florida\n",
      "29481                                          LOS ANGELES\n",
      "32823                                  Rochester, New York\n",
      "32902                               Newport News, Virginia\n",
      "35949                                          OAKLAND, CA\n",
      "84662                                Gulfport, Mississippi\n",
      "113993                                 Gramercy, Louisiana\n",
      "115999                                 Everett, Washington\n",
      "119137                        Carquinez Strait, California\n",
      "119245                                      Juneau, Alaska\n",
      "119570                                   Chicago, Illinois\n",
      "131323                                  Brunswick, Georgia\n",
      "131343                                     Freeport, Texas\n",
      "131385                               Chester, Pennsylvania\n",
      "131604                                    Galveston, Texas\n",
      "134130                              Morgan City, Louisiana\n",
      "134580                              New Haven, Connecticut\n",
      "134583                             Perth Amboy, New Jersey\n",
      "135123                                    Atlanta, Georgia\n",
      "138247                                              BOSTON\n",
      "138868                            Port Hueneme, California\n",
      "138913                               Corpus Christi, Texas\n",
      "141962                                     Cleveland, Ohio\n",
      "142274    Houston Intercontinental Airport, Houston, Texas\n",
      "142624                                          CHARLESTON\n",
      "Name: US.PORT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "### Weight KG -- Cleaned Name\n",
    "\n",
    "weight_kg = train_df_labels['WEIGHT..KG.']\n",
    "\n",
    "### US.PORT\n",
    "### One Hot Encoding\n",
    "port_one_hot = train_df_labels['US.PORT']\n",
    "port_dummies = pd.get_dummies(port_one_hot)\n",
    "print(port_one_hot.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6ab6e",
   "metadata": {},
   "source": [
    "#### Note: I'm not entirely sure why BOSTON, OAKLAND, NEW YORK, CHARLESTON are different than their counterparts but I suppose it's possible that a Capitalized place is different? Also don't know why Vancouver is included as a US Port. I chose not to remove these observations b/c I have no domain knowledge and instances seem to be rare of the dataset but it might be worth further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d19fc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-d20a5cb8b734>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['my_dates'] = pd.to_datetime(train_df_labels['ARRIVAL.DATE'])\n",
      "<ipython-input-76-d20a5cb8b734>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['day_of_week'] = train_df_labels['my_dates'].dt.day_name()\n",
      "<ipython-input-76-d20a5cb8b734>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['week_type'] = train_df_labels.apply(weekend, axis=1)\n",
      "<ipython-input-76-d20a5cb8b734>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['month'] = train_df_labels['my_dates'].dt.month_name()\n",
      "<ipython-input-76-d20a5cb8b734>:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['season'] = train_df_labels.apply(season, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SHIPPER</th>\n",
       "      <th>SHIPPER.ADDRESS</th>\n",
       "      <th>CONSIGNEE</th>\n",
       "      <th>CONSIGNEE.ADDRESS</th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>NOTIFY</th>\n",
       "      <th>NOTIFY.ADDRESS</th>\n",
       "      <th>BILL.OF.LADING</th>\n",
       "      <th>ARRIVAL.DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>CARRIER.NAME</th>\n",
       "      <th>CARRIER.ADDRESS</th>\n",
       "      <th>CARRIER.CITY</th>\n",
       "      <th>CARRIER.STATE</th>\n",
       "      <th>CARRIER.ZIP</th>\n",
       "      <th>my_dates</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week_type</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359136</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359139</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359147</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359149</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359150</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 SHIPPER SHIPPER.ADDRESS        CONSIGNEE CONSIGNEE.ADDRESS  \\\n",
       "0           1     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "1           3     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "2           6     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "3           7     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "4           8     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "\n",
       "  ZIPCODE NOTIFY NOTIFY.ADDRESS BILL.OF.LADING ARRIVAL.DATE  ...  \\\n",
       "0       0    NaN            NaN  CMDUSZ2359136   09/20/2012  ...   \n",
       "1       0    NaN            NaN  CMDUSZ2359139   09/20/2012  ...   \n",
       "2       0    NaN            NaN  CMDUSZ2359147   09/20/2012  ...   \n",
       "3       0    NaN            NaN  CMDUSZ2359149   09/20/2012  ...   \n",
       "4       0    NaN            NaN  CMDUSZ2359150   09/20/2012  ...   \n",
       "\n",
       "                       CARRIER.NAME      CARRIER.ADDRESS CARRIER.CITY  \\\n",
       "0  COMPAGNIE MARITIME D-AFFRETEMENT  5701 LAKE WRIGHT DR      NORFOLK   \n",
       "1  COMPAGNIE MARITIME D-AFFRETEMENT  5701 LAKE WRIGHT DR      NORFOLK   \n",
       "2  COMPAGNIE MARITIME D-AFFRETEMENT  5701 LAKE WRIGHT DR      NORFOLK   \n",
       "3  COMPAGNIE MARITIME D-AFFRETEMENT  5701 LAKE WRIGHT DR      NORFOLK   \n",
       "4  COMPAGNIE MARITIME D-AFFRETEMENT  5701 LAKE WRIGHT DR      NORFOLK   \n",
       "\n",
       "   CARRIER.STATE CARRIER.ZIP   my_dates day_of_week week_type      month  \\\n",
       "0             VA       23502 2012-09-20    Thursday   Weekday  September   \n",
       "1             VA       23502 2012-09-20    Thursday   Weekday  September   \n",
       "2             VA       23502 2012-09-20    Thursday   Weekday  September   \n",
       "3             VA       23502 2012-09-20    Thursday   Weekday  September   \n",
       "4             VA       23502 2012-09-20    Thursday   Weekday  September   \n",
       "\n",
       "   season  \n",
       "0    Fall  \n",
       "1    Fall  \n",
       "2    Fall  \n",
       "3    Fall  \n",
       "4    Fall  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ARRIVAL.DATE\n",
    "### Create Variables to try and Capture the Driving Factors\n",
    "\n",
    "### Ensure that ARRIVAL.DATE is actually a datetime\n",
    "train_df_labels['my_dates'] = pd.to_datetime(train_df_labels['ARRIVAL.DATE'])\n",
    "\n",
    "### DOW\n",
    "train_df_labels['day_of_week'] = train_df_labels['my_dates'].dt.day_name()\n",
    "\n",
    "### Weekend v. Weekday (couldn't find native function in Pandas)\n",
    "def weekend(row):\n",
    "    #Project\n",
    "    if row['day_of_week'] == 'Saturday' or row['day_of_week'] == 'Sunday':\n",
    "        return 'Weekend'\n",
    "    else:\n",
    "        return 'Weekday'\n",
    "\n",
    "train_df_labels['week_type'] = train_df_labels.apply(weekend, axis=1)\n",
    "\n",
    "### Turn dow and week_type into Dummies\n",
    "dow_dummies = pd.get_dummies(train_df_labels['day_of_week'])\n",
    "weektype_dummies = pd.get_dummies(train_df_labels['week_type'])\n",
    "\n",
    "### Time of Year\n",
    "train_df_labels['month'] = train_df_labels['my_dates'].dt.month_name()\n",
    "\n",
    "### Season\n",
    "def season(row):\n",
    "    #Project\n",
    "    if row['month'] == 12 or row['month'] == 1 or row['month'] == 2:\n",
    "        return 'Winter'\n",
    "    elif row['month'] == 3 or row['month'] == 4 or row['month'] == 5:\n",
    "        return 'Spring'\n",
    "    elif row['month'] == 6 or row['month'] == 7 or row['month'] == 8:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "train_df_labels['season'] = train_df_labels.apply(season, axis=1)\n",
    "\n",
    "### Turn dow and week_type into Dummies\n",
    "month_dummies = pd.get_dummies(train_df_labels['month'])\n",
    "season_dummies = pd.get_dummies(train_df_labels['season'])\n",
    "\n",
    "train_df_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8dc74",
   "metadata": {},
   "source": [
    "#### Note: I looked into a smarter way to do a date or time transformation and thought very seriously about this: https://towardsdatascience.com/cyclical-features-encoding-its-about-time-ce23581845ca . What ultimately stopped me though is the fact that I'm using XGBoost and creating two variables as opposed to one can be problematic for this algo b/c \"they will fail to process these two features simultaneously whereas the cos/sin values are expected to be considered as one single coordinates system.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2f298",
   "metadata": {},
   "source": [
    "### Build Train Test off of Training Data Provided\n",
    "#### Leave Validation Dataset Given by Altana for Final Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bfe6307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-32af7bdaacc3>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['COUNTRY.OF.ORIGIN'] = train_df_labels['COUNTRY.OF.ORIGIN'].astype('category')\n",
      "<ipython-input-77-32af7bdaacc3>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['COUNTRY.OF.ORIGIN_CAT'] = train_df_labels['COUNTRY.OF.ORIGIN'].cat.codes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WEIGHT..KG.</th>\n",
       "      <th>Atlanta, Georgia</th>\n",
       "      <th>BOSTON</th>\n",
       "      <th>Baltimore, Maryland</th>\n",
       "      <th>Boston, Massachusetts</th>\n",
       "      <th>Brunswick, Georgia</th>\n",
       "      <th>CHARLESTON</th>\n",
       "      <th>Carquinez Strait, California</th>\n",
       "      <th>Charleston, South Carolina</th>\n",
       "      <th>Chester, Pennsylvania</th>\n",
       "      <th>...</th>\n",
       "      <th>February</th>\n",
       "      <th>January</th>\n",
       "      <th>July</th>\n",
       "      <th>June</th>\n",
       "      <th>March</th>\n",
       "      <th>May</th>\n",
       "      <th>November</th>\n",
       "      <th>October</th>\n",
       "      <th>September</th>\n",
       "      <th>Fall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.742400e+04</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.0</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.000000</td>\n",
       "      <td>87424.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.891680e+04</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.017695</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123353</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>0.079635</td>\n",
       "      <td>0.029877</td>\n",
       "      <td>0.078617</td>\n",
       "      <td>0.057135</td>\n",
       "      <td>0.079658</td>\n",
       "      <td>0.120836</td>\n",
       "      <td>0.069089</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.837378e+05</td>\n",
       "      <td>0.009566</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.035770</td>\n",
       "      <td>0.026406</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>0.131842</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328844</td>\n",
       "      <td>0.306297</td>\n",
       "      <td>0.270729</td>\n",
       "      <td>0.170250</td>\n",
       "      <td>0.269141</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.270764</td>\n",
       "      <td>0.325939</td>\n",
       "      <td>0.253606</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.190000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.765950e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.096025e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.050000e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        WEIGHT..KG.  Atlanta, Georgia        BOSTON  Baltimore, Maryland  \\\n",
       "count  8.742400e+04      87424.000000  87424.000000         87424.000000   \n",
       "mean   1.891680e+04          0.000092      0.000011             0.001281   \n",
       "std    5.837378e+05          0.009566      0.003382             0.035770   \n",
       "min    0.000000e+00          0.000000      0.000000             0.000000   \n",
       "25%    6.190000e+02          0.000000      0.000000             0.000000   \n",
       "50%    2.765950e+03          0.000000      0.000000             0.000000   \n",
       "75%    1.096025e+04          0.000000      0.000000             0.000000   \n",
       "max    6.050000e+07          1.000000      1.000000             1.000000   \n",
       "\n",
       "       Boston, Massachusetts  Brunswick, Georgia  CHARLESTON  \\\n",
       "count           87424.000000        87424.000000     87424.0   \n",
       "mean                0.000698            0.000023         0.0   \n",
       "std                 0.026406            0.004783         0.0   \n",
       "min                 0.000000            0.000000         0.0   \n",
       "25%                 0.000000            0.000000         0.0   \n",
       "50%                 0.000000            0.000000         0.0   \n",
       "75%                 0.000000            0.000000         0.0   \n",
       "max                 1.000000            1.000000         0.0   \n",
       "\n",
       "       Carquinez Strait, California  Charleston, South Carolina  \\\n",
       "count                  87424.000000                87424.000000   \n",
       "mean                       0.000011                    0.017695   \n",
       "std                        0.003382                    0.131842   \n",
       "min                        0.000000                    0.000000   \n",
       "25%                        0.000000                    0.000000   \n",
       "50%                        0.000000                    0.000000   \n",
       "75%                        0.000000                    0.000000   \n",
       "max                        1.000000                    1.000000   \n",
       "\n",
       "       Chester, Pennsylvania  ...      February       January          July  \\\n",
       "count           87424.000000  ...  87424.000000  87424.000000  87424.000000   \n",
       "mean                0.000160  ...      0.123353      0.104800      0.079635   \n",
       "std                 0.012654  ...      0.328844      0.306297      0.270729   \n",
       "min                 0.000000  ...      0.000000      0.000000      0.000000   \n",
       "25%                 0.000000  ...      0.000000      0.000000      0.000000   \n",
       "50%                 0.000000  ...      0.000000      0.000000      0.000000   \n",
       "75%                 0.000000  ...      0.000000      0.000000      0.000000   \n",
       "max                 1.000000  ...      1.000000      1.000000      1.000000   \n",
       "\n",
       "               June         March           May      November       October  \\\n",
       "count  87424.000000  87424.000000  87424.000000  87424.000000  87424.000000   \n",
       "mean       0.029877      0.078617      0.057135      0.079658      0.120836   \n",
       "std        0.170250      0.269141      0.232102      0.270764      0.325939   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          September     Fall  \n",
       "count  87424.000000  87424.0  \n",
       "mean       0.069089      1.0  \n",
       "std        0.253606      0.0  \n",
       "min        0.000000      1.0  \n",
       "25%        0.000000      1.0  \n",
       "50%        0.000000      1.0  \n",
       "75%        0.000000      1.0  \n",
       "max        1.000000      1.0  \n",
       "\n",
       "[8 rows x 81 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Combine all features Together into one training dataset\n",
    "### Class Labels Need to Be Numeric\n",
    "train_df_labels['COUNTRY.OF.ORIGIN'] = train_df_labels['COUNTRY.OF.ORIGIN'].astype('category')\n",
    "train_df_labels['COUNTRY.OF.ORIGIN_CAT'] = train_df_labels['COUNTRY.OF.ORIGIN'].cat.codes\n",
    "\n",
    "### Xs are Featured Engineered Above and y is Numeric Category Codes from Country \n",
    "y = train_df_labels['COUNTRY.OF.ORIGIN_CAT']\n",
    "X = pd.concat([weight_kg,port_dummies,dow_dummies,weektype_dummies,month_dummies,season_dummies], axis=1)\n",
    "\n",
    "### Create Training and Testing Sets 2/3rds (train) v. 1/3 (holdout)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2adc2",
   "metadata": {},
   "source": [
    "#### Fit XGBoost Classifier -- Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c19f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:57:09] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 72.05%\n"
     ]
    }
   ],
   "source": [
    "### No Rebalancing of the Classes and Keeping all Classes\n",
    "\n",
    "#Import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier() \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test) \n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions) \n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca89a30",
   "metadata": {},
   "source": [
    "#### I was a bit worried about runtime but it wasn't too bad -- about 7 minutes run locally on my personal Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0951885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.92        81\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.56      0.31      0.40        78\n",
      "           4       0.21      0.06      0.10        62\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.47      0.29      0.36        89\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.50      0.38      0.43        37\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.79      0.81      0.80       254\n",
      "          12       0.82      0.88      0.85     31203\n",
      "          13       0.16      0.14      0.15       727\n",
      "          14       0.77      0.30      0.43        33\n",
      "          15       0.99      0.98      0.99       413\n",
      "          17       0.44      0.16      0.23        45\n",
      "          18       0.00      0.00      0.00         2\n",
      "          19       0.08      0.06      0.07        84\n",
      "          21       0.33      0.17      0.22        18\n",
      "          22       0.29      0.21      0.25       201\n",
      "          23       0.67      0.50      0.57        20\n",
      "          26       0.78      0.72      0.75       553\n",
      "          27       0.29      0.17      0.22        23\n",
      "          28       0.13      0.05      0.08        75\n",
      "          29       0.13      0.10      0.11      1634\n",
      "          30       1.00      1.00      1.00         1\n",
      "          31       0.23      0.23      0.23       389\n",
      "          32       0.50      1.00      0.67         1\n",
      "          33       0.29      0.16      0.21        25\n",
      "          34       0.48      0.27      0.34       105\n",
      "          35       0.24      0.13      0.17        68\n",
      "          36       0.33      0.25      0.28       243\n",
      "          37       0.00      0.00      0.00         9\n",
      "          39       0.23      0.12      0.16       684\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.46      0.34      0.39       120\n",
      "          42       1.00      0.33      0.50         3\n",
      "          44       0.50      0.15      0.23        41\n",
      "          45       0.00      0.00      0.00         2\n",
      "          46       0.34      0.35      0.34       731\n",
      "          47       0.12      0.07      0.09       208\n",
      "          48       0.47      0.43      0.45       750\n",
      "          49       0.00      0.00      0.00         4\n",
      "          51       0.00      0.00      0.00        12\n",
      "          52       0.47      0.39      0.42        18\n",
      "          53       0.00      0.00      0.00         2\n",
      "          54       0.39      0.35      0.37      1353\n",
      "          55       0.67      0.74      0.70        19\n",
      "          56       0.20      0.14      0.16      1416\n",
      "          57       0.57      0.58      0.57       474\n",
      "          58       0.11      0.09      0.10       243\n",
      "          59       0.71      0.86      0.78        43\n",
      "          61       0.25      0.24      0.24       202\n",
      "          62       0.00      0.00      0.00         3\n",
      "          63       0.00      0.00      0.00        11\n",
      "          65       1.00      0.60      0.75         5\n",
      "          66       0.50      0.25      0.33        64\n",
      "          67       0.00      0.00      0.00         5\n",
      "          68       0.00      0.00      0.00         1\n",
      "          69       0.16      0.09      0.12       152\n",
      "\n",
      "    accuracy                           0.72     43061\n",
      "   macro avg       0.34      0.27      0.29     43061\n",
      "weighted avg       0.69      0.72      0.70     43061\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# df1 = train_df_labels.sort_values('COUNTRY.OF.ORIGIN_CAT', ascending=True).drop_duplicates('COUNTRY.OF.ORIGIN').sort_index()\n",
    "# country_key = df1[['COUNTRY.OF.ORIGIN_CAT','COUNTRY.OF.ORIGIN']].sort_values('COUNTRY.OF.ORIGIN_CAT', ascending=True)\n",
    "# country_key_values = country_key['COUNTRY.OF.ORIGIN'].tolist()\n",
    "# len(country_key_values)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b676c",
   "metadata": {},
   "source": [
    "### In analyzing the above classification report I think there are a few takeaways:\n",
    "##### 1) XGBoost does a decent job training and predicting with a relatively simple set of simple features \n",
    "##### 2) .72 accuracy is a nice place to start and will serve as a baseline to compare to future models and improvements\n",
    "##### 3) We need to trim out countries with very low data as it's going to be difficult to predict with this information. While it's somewhat arbitrary I think combining countries with less than 10 observations is a good place to start. In 1/3 vs 2/3rd train test scenario the training set will likely be too small to actually learn differences b/w classes\n",
    "##### 4) I think a SMOTE procedure or some hyperparameter tuning around the XGBoost class weights will suit this data well given the dominance of China in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "905b9ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-3f9adcf8878d>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['COUNTRY.OF.ORIGIN_TRIM'] = train_df_labels.apply(country_other, axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SHIPPER</th>\n",
       "      <th>SHIPPER.ADDRESS</th>\n",
       "      <th>CONSIGNEE</th>\n",
       "      <th>CONSIGNEE.ADDRESS</th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>NOTIFY</th>\n",
       "      <th>NOTIFY.ADDRESS</th>\n",
       "      <th>BILL.OF.LADING</th>\n",
       "      <th>ARRIVAL.DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>CARRIER.CITY</th>\n",
       "      <th>CARRIER.STATE</th>\n",
       "      <th>CARRIER.ZIP</th>\n",
       "      <th>my_dates</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week_type</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>COUNTRY.OF.ORIGIN_CAT</th>\n",
       "      <th>COUNTRY.OF.ORIGIN_TRIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359136</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>12</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359139</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>12</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359147</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>12</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359149</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>12</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359150</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>12</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 SHIPPER SHIPPER.ADDRESS        CONSIGNEE CONSIGNEE.ADDRESS  \\\n",
       "0           1     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "1           3     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "2           6     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "3           7     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "4           8     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "\n",
       "  ZIPCODE NOTIFY NOTIFY.ADDRESS BILL.OF.LADING ARRIVAL.DATE  ...  \\\n",
       "0       0    NaN            NaN  CMDUSZ2359136   09/20/2012  ...   \n",
       "1       0    NaN            NaN  CMDUSZ2359139   09/20/2012  ...   \n",
       "2       0    NaN            NaN  CMDUSZ2359147   09/20/2012  ...   \n",
       "3       0    NaN            NaN  CMDUSZ2359149   09/20/2012  ...   \n",
       "4       0    NaN            NaN  CMDUSZ2359150   09/20/2012  ...   \n",
       "\n",
       "   CARRIER.CITY  CARRIER.STATE CARRIER.ZIP   my_dates day_of_week  week_type  \\\n",
       "0       NORFOLK             VA       23502 2012-09-20    Thursday    Weekday   \n",
       "1       NORFOLK             VA       23502 2012-09-20    Thursday    Weekday   \n",
       "2       NORFOLK             VA       23502 2012-09-20    Thursday    Weekday   \n",
       "3       NORFOLK             VA       23502 2012-09-20    Thursday    Weekday   \n",
       "4       NORFOLK             VA       23502 2012-09-20    Thursday    Weekday   \n",
       "\n",
       "       month season COUNTRY.OF.ORIGIN_CAT  COUNTRY.OF.ORIGIN_TRIM  \n",
       "0  September   Fall                    12                   China  \n",
       "1  September   Fall                    12                   China  \n",
       "2  September   Fall                    12                   China  \n",
       "3  September   Fall                    12                   China  \n",
       "4  September   Fall                    12                   China  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Trimming Down the Classes to Less than or Equal to 10 Observations\n",
    "\n",
    "### Country_Other\n",
    "def country_other(row):\n",
    "    #Project\n",
    "    if row['COUNTRY.OF.ORIGIN'] in ['Guadeloupe','Kuwait','Denmark','Cayman Isl','Guam','Belize',\n",
    "                                    'Philippines','Federal Republic of Germany','US Virgin Is','Taiwan',\n",
    "                                    'Neth Antilles','Cambodia','Saudi Arabia','Venezuela','Malta','Iceland','Ecuador',\n",
    "                                    'New Zealand','Bermuda','Indonesia','Morocco','Trinidad','Peru']:\n",
    "        return 'Other'\n",
    "    else:\n",
    "        return row['COUNTRY.OF.ORIGIN']\n",
    "\n",
    "train_df_labels['COUNTRY.OF.ORIGIN_TRIM'] = train_df_labels.apply(country_other, axis=1)\n",
    "\n",
    "train_df_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9f83a",
   "metadata": {},
   "source": [
    "### Refit Data with fewer classes -- Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c216a71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-e38112ce42ca>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['COUNTRY.OF.ORIGIN_TRIM'] = train_df_labels['COUNTRY.OF.ORIGIN_TRIM'].astype('category')\n",
      "<ipython-input-79-e38112ce42ca>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_labels['COUNTRY.OF.ORIGIN_CAT2'] = train_df_labels['COUNTRY.OF.ORIGIN_TRIM'].cat.codes\n"
     ]
    }
   ],
   "source": [
    "### Combine all features Together into one training dataset\n",
    "\n",
    "### Class Labels Need to Be Numeric Based on Updated Information _TRIM VAR\n",
    "train_df_labels['COUNTRY.OF.ORIGIN_TRIM'] = train_df_labels['COUNTRY.OF.ORIGIN_TRIM'].astype('category')\n",
    "train_df_labels['COUNTRY.OF.ORIGIN_CAT2'] = train_df_labels['COUNTRY.OF.ORIGIN_TRIM'].cat.codes\n",
    "\n",
    "### Xs are Featured Engineered Above and y is Numeric Category Codes from Country \n",
    "y = train_df_labels['COUNTRY.OF.ORIGIN_CAT2']\n",
    "X = pd.concat([weight_kg,port_dummies,dow_dummies,weektype_dummies,month_dummies,season_dummies], axis=1)\n",
    "\n",
    "### Create Training and Testing Sets 2/3rds (train) v. 1/3 (holdout)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a563e4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:12:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 81.50%\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier() \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test) \n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions) \n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7aa9d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    American Samoa       0.94      0.84      0.89        81\n",
      "         Argentina       0.60      0.21      0.32        14\n",
      "         Australia       0.00      0.00      0.00         3\n",
      "           Bahamas       0.72      0.37      0.49        78\n",
      "           Belgium       0.36      0.15      0.21        62\n",
      "            Brazil       0.72      0.33      0.45        89\n",
      "            Canada       0.75      0.49      0.59        37\n",
      "             Chile       0.86      0.84      0.85       254\n",
      "             China       0.85      0.98      0.91     31203\n",
      "      China Taiwan       0.56      0.29      0.38       727\n",
      "          Colombia       0.75      0.36      0.49        33\n",
      "        Costa Rica       0.98      0.99      0.99       413\n",
      "Dominican Republic       0.70      0.42      0.53        45\n",
      "             Egypt       0.52      0.15      0.24        84\n",
      "            France       0.50      0.28      0.36        18\n",
      "           Germany       0.62      0.36      0.45       201\n",
      "            Greece       0.61      0.55      0.58        20\n",
      "         Guatemala       0.92      0.82      0.87       553\n",
      "             Haiti       0.67      0.61      0.64        23\n",
      "          Honduras       0.45      0.12      0.19        75\n",
      "         Hong Kong       0.67      0.13      0.21      1634\n",
      "             India       0.42      0.30      0.35       389\n",
      "            Israel       0.44      0.16      0.24        25\n",
      "             Italy       0.62      0.34      0.44       105\n",
      "           Jamaica       0.73      0.32      0.45        68\n",
      "             Japan       0.67      0.38      0.48       243\n",
      "            Jordan       0.60      0.33      0.43         9\n",
      "          Malaysia       0.54      0.14      0.22       684\n",
      "            Mexico       0.64      0.57      0.60       120\n",
      "       Netherlands       0.67      0.10      0.17        41\n",
      "              Oman       0.56      0.56      0.56       731\n",
      "             Other       0.75      0.12      0.21        24\n",
      "          Pakistan       0.43      0.14      0.22       208\n",
      "            Panama       0.65      0.64      0.65       750\n",
      "          Portugal       0.00      0.00      0.00        12\n",
      "           Romania       0.50      0.39      0.44        18\n",
      "         Singapore       0.59      0.49      0.54      1353\n",
      "      South Africa       0.82      0.74      0.78        19\n",
      "       South Korea       0.62      0.18      0.28      1416\n",
      "             Spain       0.63      0.61      0.62       474\n",
      "         Sri Lanka       0.43      0.16      0.23       243\n",
      "            Sweden       0.80      0.86      0.83        43\n",
      "          Thailand       0.55      0.28      0.37       202\n",
      "            Turkey       0.00      0.00      0.00        11\n",
      "    United Arab Em       0.75      0.60      0.67         5\n",
      "    United Kingdom       0.61      0.36      0.45        64\n",
      "           Unknown       0.50      0.40      0.44         5\n",
      "           Vietnam       0.60      0.16      0.26       152\n",
      "\n",
      "          accuracy                           0.81     43061\n",
      "         macro avg       0.60      0.39      0.45     43061\n",
      "      weighted avg       0.79      0.81      0.78     43061\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import classification_report\n",
    "\n",
    "#Get the Labels in the Report\n",
    "df1 = train_df_labels.sort_values('COUNTRY.OF.ORIGIN_CAT2', ascending=True).drop_duplicates('COUNTRY.OF.ORIGIN_TRIM').sort_index()\n",
    "country_key = df1[['COUNTRY.OF.ORIGIN_CAT2','COUNTRY.OF.ORIGIN_TRIM']].sort_values('COUNTRY.OF.ORIGIN_CAT2', ascending=True)\n",
    "country_key_values = country_key['COUNTRY.OF.ORIGIN_TRIM'].tolist()\n",
    "#len(country_key_values)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=country_key_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f27d6",
   "metadata": {},
   "source": [
    "#### Not suprisingly, when we take the countries that have less than or equal to 10 total observations and combine them into an \"other\" group our model performs much better overall. This is to be expected as we combined 23 countries together but it's still nice to see. Individual F1 Scores still leave something to be desired but we're improving. Some countries are wayyyy better than others (besides China). For example, Costa Rica is very accurate (F1 Score) while Italy isn't (F1 Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d703ec9",
   "metadata": {},
   "source": [
    "### Rebalancing the Training Dataset using SMOTE to try and oversample minority classes and hopefully lesson the China dominant effect. Model -- 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "21430290",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With under-sampling methods, the number of samples in a class should be less or equal to the original number of samples. Originally, there is 171 samples and 1000 samples are asked.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-bd9bf5e1e9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             41:1000,42:1000,43:1000,44:1000,45:1000,46:1000,47:1000}\n\u001b[1;32m     12\u001b[0m \u001b[0mundersample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mundersample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# summarize distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         self.sampling_strategy_ = check_sampling_strategy(\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36mcheck_sampling_strategy\u001b[0;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         return OrderedDict(\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sampling_strategy_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         )\n\u001b[1;32m    523\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36m_sampling_strategy_dict\u001b[0;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampling_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtarget_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    310\u001b[0m                     \u001b[0;34mf\"With under-sampling methods, the number of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34mf\" samples in a class should be less or equal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: With under-sampling methods, the number of samples in a class should be less or equal to the original number of samples. Originally, there is 171 samples and 1000 samples are asked."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# transform the dataset\n",
    "strategy = {0:1000,1:1000,2:1000,3:1000,4:1000,5:1000,6:1000,7:1000,8:10000,9:1000,10:1000,\n",
    "            11:1000,12:1000,13:1000,14:1000,15:1000,16:1000,17:1000,18:1000,19:1000,20:1634,\n",
    "            21:1000,22:1000,23:1000,24:1000,25:1000,26:1000,27:1000,28:1000,29:1000,30:1000,\n",
    "            31:1000,32:1000,33:1000,34:1000,35:1000,36:1353,37:1000,38:1416,39:1000,40:1000,\n",
    "            41:1000,42:1000,43:1000,44:1000,45:1000,46:1000,47:1000}\n",
    "undersample = RandomUnderSampler(sampling_strategy=strategy)\n",
    "X_train, y_train = undersample.fit_resample(X_train, y_train)  \n",
    "# summarize distribution\n",
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y_train) * 100\n",
    "    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "pyplot.bar(counter.keys(), counter.values())\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d524a",
   "metadata": {},
   "source": [
    "#### I tried an oversample technique but I don't have the compute on my personal Mac to execute it properly. Therefore, I will try an undersampling technique instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "168e491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:02:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 78.31%\n"
     ]
    }
   ],
   "source": [
    "#Import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier() \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test) \n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions) \n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e218489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    American Samoa       0.83      0.89      0.86        81\n",
      "         Argentina       0.16      0.36      0.22        14\n",
      "         Australia       0.08      0.33      0.13         3\n",
      "           Bahamas       0.32      0.35      0.33        78\n",
      "           Belgium       0.13      0.18      0.15        62\n",
      "            Brazil       0.42      0.34      0.37        89\n",
      "            Canada       0.36      0.49      0.41        37\n",
      "             Chile       0.80      0.81      0.81       254\n",
      "             China       0.83      0.97      0.89     31203\n",
      "      China Taiwan       0.40      0.12      0.19       727\n",
      "          Colombia       0.31      0.39      0.35        33\n",
      "        Costa Rica       0.97      0.99      0.98       413\n",
      "Dominican Republic       0.46      0.64      0.54        45\n",
      "             Egypt       0.24      0.33      0.28        84\n",
      "            France       0.09      0.33      0.14        18\n",
      "           Germany       0.42      0.25      0.31       201\n",
      "            Greece       0.39      0.65      0.49        20\n",
      "         Guatemala       0.89      0.73      0.80       553\n",
      "             Haiti       0.26      0.83      0.39        23\n",
      "          Honduras       0.28      0.13      0.18        75\n",
      "         Hong Kong       0.56      0.07      0.13      1634\n",
      "             India       0.30      0.12      0.17       389\n",
      "            Israel       0.14      0.36      0.20        25\n",
      "             Italy       0.45      0.28      0.34       105\n",
      "           Jamaica       0.25      0.37      0.30        68\n",
      "             Japan       0.53      0.35      0.42       243\n",
      "            Jordan       0.09      0.67      0.15         9\n",
      "          Malaysia       0.49      0.12      0.19       684\n",
      "            Mexico       0.47      0.71      0.57       120\n",
      "       Netherlands       0.14      0.12      0.13        41\n",
      "              Oman       0.59      0.26      0.36       731\n",
      "             Other       0.15      0.17      0.16        24\n",
      "          Pakistan       0.22      0.18      0.20       208\n",
      "            Panama       0.71      0.46      0.56       750\n",
      "          Portugal       0.02      0.08      0.03        12\n",
      "           Romania       0.10      0.22      0.14        18\n",
      "         Singapore       0.59      0.30      0.40      1353\n",
      "      South Africa       0.21      0.84      0.33        19\n",
      "       South Korea       0.64      0.13      0.22      1416\n",
      "             Spain       0.62      0.46      0.53       474\n",
      "         Sri Lanka       0.36      0.10      0.16       243\n",
      "            Sweden       0.31      0.95      0.47        43\n",
      "          Thailand       0.23      0.28      0.25       202\n",
      "            Turkey       0.00      0.00      0.00        11\n",
      "    United Arab Em       0.23      0.60      0.33         5\n",
      "    United Kingdom       0.25      0.23      0.24        64\n",
      "           Unknown       0.40      0.40      0.40         5\n",
      "           Vietnam       0.16      0.13      0.14       152\n",
      "\n",
      "          accuracy                           0.78     43061\n",
      "         macro avg       0.37      0.40      0.34     43061\n",
      "      weighted avg       0.75      0.78      0.74     43061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Get the Labels in the Report\n",
    "df1 = train_df_labels.sort_values('COUNTRY.OF.ORIGIN_CAT2', ascending=True).drop_duplicates('COUNTRY.OF.ORIGIN_TRIM').sort_index()\n",
    "country_key = df1[['COUNTRY.OF.ORIGIN_CAT2','COUNTRY.OF.ORIGIN_TRIM']].sort_values('COUNTRY.OF.ORIGIN_CAT2', ascending=True)\n",
    "country_key_values = country_key['COUNTRY.OF.ORIGIN_TRIM'].tolist()\n",
    "#len(country_key_values)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=country_key_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcd966",
   "metadata": {},
   "source": [
    "#### Surprisingly, undersampling China and oversampling the other classes hurts the overall model. I do believe there is ALOT more work to be done in researching this but in the spirit on the \"3 hour takehome\" I'll stop and won't go down the deep rabbit hole of differnt reblancing strategies of the classes. I'm also disappointed my compute power can't fully balance all classes equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98def8c5",
   "metadata": {},
   "source": [
    "### Using NLP to Try and Extract Signal out of the Product Details Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "977f83e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '00007', '00014', '00016', '00018', '00025', '00063', '0020', '0023', '0040', '0042', '0043', '0073', '0078742000008', '10', '100', '12', '13', '16', '20', '2012', '2013', '24', '30', '40', '43', '48', '5oz', '9503', '9503000073', 'allowed', 'artificial', 'asst', 'black', 'bx', 'canada', 'carrier', 'cartons', 'cbm', 'certify', 'chair', 'collect', 'com', 'consistent', 'copy', 'cotton', 'ctn', 'ctns', 'cy', 'declaration', 'declare', 'details', 'elwood', 'expeditors', 'fg', 'flow', 'footwear', 'freight', 'fresh', 'furniture', 'gln', 'great', 'houston', 'hs', 'hts', 'issue', 'itemized', 'kgs', 'knit', 'ladies', 'load', 'ms', 'op', 'packing', 'pam', 'place', 'plastic', 'polyester', 'pono', 'regulated', 'release', 'savannah', 'sea', 'set', 'sk', 'solid', 'southgate', 'steel', 'stock', 'storage', 'style', 'suffolk', 'toys', 'value', 'vancouver', 'virtual', 'wal', 'way', 'white', 'wood']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "### Expand the StopWords List by Eyeballing things that don't make sense\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['item','vendor','type','po','dept','description',\n",
    "                                            'pls','cargo','assortment','container','contains',\n",
    "                                            'material','materials','shipment','shipper','qty',\n",
    "                                            'purchase','product','procedures','pcs','order',\n",
    "                                            'destination','delivery','count','code','ct',\n",
    "                                            'department','negotiable','negotiated','non',\n",
    "                                            'number','packaging','supplier'])\n",
    "\n",
    "corpus = train_df_labels['PRODUCT.DETAILS'].values.astype('U')\n",
    "cv = CountVectorizer(max_features = 100, stop_words=stop_words)\n",
    "product_details_parsed = cv.fit_transform(corpus)\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e55098",
   "metadata": {},
   "source": [
    "#### It's hard to know that the right cutoff should be in terms of number of NLP bag of words features. We know XGBoost will guard against overfitting so it might just be a matter of compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "045f5ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00007</th>\n",
       "      <th>00014</th>\n",
       "      <th>00016</th>\n",
       "      <th>00018</th>\n",
       "      <th>00025</th>\n",
       "      <th>00063</th>\n",
       "      <th>0020</th>\n",
       "      <th>0023</th>\n",
       "      <th>0040</th>\n",
       "      <th>...</th>\n",
       "      <th>style</th>\n",
       "      <th>suffolk</th>\n",
       "      <th>toys</th>\n",
       "      <th>value</th>\n",
       "      <th>vancouver</th>\n",
       "      <th>virtual</th>\n",
       "      <th>wal</th>\n",
       "      <th>way</th>\n",
       "      <th>white</th>\n",
       "      <th>wood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  00007  00014  00016  00018  00025  00063  0020  0023  0040  ...  style  \\\n",
       "0   0      0      0      0      0      0      0     0     0     0  ...      0   \n",
       "1   0      0      0      0      0      0      0     0     0     0  ...      0   \n",
       "2   0      0      0      0      0      0      0     0     0     0  ...      0   \n",
       "3   0      0      0      0      0      0      0     0     0     0  ...      0   \n",
       "4   0      0      0      0      0      0      0     0     0     0  ...      0   \n",
       "\n",
       "   suffolk  toys  value  vancouver  virtual  wal  way  white  wood  \n",
       "0        0     0      0          0        0    0    0      0     1  \n",
       "1        0     0      0          0        0    0    0      0     0  \n",
       "2        0     0      0          0        0    0    0      0     0  \n",
       "3        0     0      0          0        0    0    0      0     0  \n",
       "4        0     0      0          0        0    0    0      0     1  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Turn Product Details into a DF and \n",
    "df_nlp = pd.DataFrame(product_details_parsed.todense(), columns=cv.get_feature_names())\n",
    "\n",
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e933c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update X to include the NLP Vectorized Features\n",
    "\n",
    "X1 = pd.concat([weight_kg,port_dummies,dow_dummies,weektype_dummies,month_dummies,season_dummies], axis=1).reset_index()\n",
    "X = pd.concat([X1,df_nlp], axis=1)\n",
    "\n",
    "#Create Training and Testing Sets 2/3rds (train) v. 1/3 (holdout)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea506f6",
   "metadata": {},
   "source": [
    "### Fit Model Using the NLP Bag of Words Features with No Class Rebalancing -- Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a84ad129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:00:36] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 92.96%\n"
     ]
    }
   ],
   "source": [
    "### Fit XGboost\n",
    "\n",
    "#Import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier() \n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test) \n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions) \n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e358f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    American Samoa       1.00      0.96      0.98        81\n",
      "         Argentina       1.00      0.57      0.73        14\n",
      "         Australia       1.00      0.33      0.50         3\n",
      "           Bahamas       0.78      0.64      0.70        78\n",
      "           Belgium       0.80      0.63      0.70        62\n",
      "            Brazil       0.89      0.88      0.88        89\n",
      "            Canada       0.88      0.59      0.71        37\n",
      "             Chile       0.94      0.97      0.95       254\n",
      "             China       0.95      0.99      0.97     31203\n",
      "      China Taiwan       0.84      0.73      0.78       727\n",
      "          Colombia       0.81      0.79      0.80        33\n",
      "        Costa Rica       0.99      1.00      1.00       413\n",
      "Dominican Republic       0.94      0.71      0.81        45\n",
      "             Egypt       0.96      0.96      0.96        84\n",
      "            France       0.78      0.39      0.52        18\n",
      "           Germany       0.81      0.82      0.81       201\n",
      "            Greece       0.76      0.65      0.70        20\n",
      "         Guatemala       0.99      0.98      0.98       553\n",
      "             Haiti       1.00      0.96      0.98        23\n",
      "          Honduras       0.95      0.93      0.94        75\n",
      "         Hong Kong       0.83      0.56      0.67      1634\n",
      "             India       0.87      0.82      0.85       389\n",
      "            Israel       0.80      0.32      0.46        25\n",
      "             Italy       0.68      0.65      0.66       105\n",
      "           Jamaica       0.92      0.65      0.76        68\n",
      "             Japan       0.91      0.87      0.89       243\n",
      "            Jordan       1.00      0.78      0.88         9\n",
      "          Malaysia       0.90      0.81      0.85       684\n",
      "            Mexico       0.97      0.94      0.96       120\n",
      "       Netherlands       0.89      0.39      0.54        41\n",
      "              Oman       0.91      0.86      0.88       731\n",
      "             Other       1.00      0.25      0.40        24\n",
      "          Pakistan       0.89      0.84      0.86       208\n",
      "            Panama       0.94      0.89      0.91       750\n",
      "          Portugal       0.67      0.17      0.27        12\n",
      "           Romania       0.86      0.67      0.75        18\n",
      "         Singapore       0.81      0.79      0.80      1353\n",
      "      South Africa       1.00      1.00      1.00        19\n",
      "       South Korea       0.84      0.73      0.78      1416\n",
      "             Spain       0.88      0.88      0.88       474\n",
      "         Sri Lanka       0.91      0.85      0.88       243\n",
      "            Sweden       1.00      0.98      0.99        43\n",
      "          Thailand       0.78      0.62      0.69       202\n",
      "            Turkey       1.00      0.45      0.62        11\n",
      "    United Arab Em       1.00      0.40      0.57         5\n",
      "    United Kingdom       0.81      0.69      0.75        64\n",
      "           Unknown       1.00      0.80      0.89         5\n",
      "           Vietnam       0.79      0.71      0.75       152\n",
      "\n",
      "          accuracy                           0.93     43061\n",
      "         macro avg       0.89      0.73      0.78     43061\n",
      "      weighted avg       0.93      0.93      0.93     43061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Get the Labels in the Report\n",
    "df1 = train_df_labels.sort_values('COUNTRY.OF.ORIGIN_CAT2', ascending=True).drop_duplicates('COUNTRY.OF.ORIGIN_TRIM').sort_index()\n",
    "country_key = df1[['COUNTRY.OF.ORIGIN_CAT2','COUNTRY.OF.ORIGIN_TRIM']].sort_values('COUNTRY.OF.ORIGIN_CAT2', ascending=True)\n",
    "country_key_values = country_key['COUNTRY.OF.ORIGIN_TRIM'].tolist()\n",
    "#len(country_key_values)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=country_key_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0918e5",
   "metadata": {},
   "source": [
    "#### With even the most basic NLP (Bag of Words) the improvement in the model is significant our accuracy goes to .93% and the F1 scores for some smaller countries are significantly improved. I would imagine that more advanced approaches like tf idf, word to vec, BERT Library could push performance even higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad87d8a",
   "metadata": {},
   "source": [
    "### Read in Validation Model, Prepare Data and Predict (Using Trained Model 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a27db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   Long Beach, California\n",
      "4                                    Jacksonville, Florida\n",
      "5                                        Norfolk, Virginia\n",
      "12                                       Savannah, Georgia\n",
      "13                                          Houston, Texas\n",
      "15                                     Seattle, Washington\n",
      "52                                 Los Angeles, California\n",
      "397                                     Tacoma, Washington\n",
      "513                             Philadelphia, Pennsylvania\n",
      "521               New York/Newark Area, Newark, New Jersey\n",
      "559                             Charleston, South Carolina\n",
      "670                                  San Juan, Puerto Rico\n",
      "1732                                    New York, New York\n",
      "3062                              Port Everglades, Florida\n",
      "3100                                                   NaN\n",
      "3119                                  Wilmington, Delaware\n",
      "3152                                        Miami, Florida\n",
      "3157                                   Oakland, California\n",
      "3333                                        Juneau, Alaska\n",
      "3689                                New Orleans, Louisiana\n",
      "4589                               Port Canaveral, Florida\n",
      "4632                                      Portland, Oregon\n",
      "4913                                   Baltimore, Maryland\n",
      "7075                                       Mobile, Alabama\n",
      "8137                                 Boston, Massachusetts\n",
      "8612                                      Galveston, Texas\n",
      "8638                                      Honolulu, Hawaii\n",
      "8686                              West Palm Beach, Florida\n",
      "9872                                           LOS ANGELES\n",
      "9891                                              NEW YORK\n",
      "10952                                         VANCOUVER BC\n",
      "29381                                Gulfport, Mississippi\n",
      "38045                                  Gramercy, Louisiana\n",
      "38190                                San Diego, California\n",
      "38293                            San Francisco, California\n",
      "38882                              Lawrence, Massachusetts\n",
      "38944                                   Newark, New Jersey\n",
      "39270                                    Chicago, Illinois\n",
      "39351                           Wilmington, North Carolina\n",
      "43791                                Chester, Pennsylvania\n",
      "43794                                       Tampa, Florida\n",
      "43881                                  Everett, Washington\n",
      "45055                                     Atlanta, Georgia\n",
      "45321                             Port Hueneme, California\n",
      "45784    Los Angeles International Airport, Los Angeles...\n",
      "45863                                Corpus Christi, Texas\n",
      "47656                                      Cleveland, Ohio\n",
      "Name: US.PORT, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SHIPPER</th>\n",
       "      <th>SHIPPER.ADDRESS</th>\n",
       "      <th>CONSIGNEE</th>\n",
       "      <th>CONSIGNEE.ADDRESS</th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>NOTIFY</th>\n",
       "      <th>NOTIFY.ADDRESS</th>\n",
       "      <th>BILL.OF.LADING</th>\n",
       "      <th>ARRIVAL.DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>CARRIER.NAME</th>\n",
       "      <th>CARRIER.ADDRESS</th>\n",
       "      <th>CARRIER.CITY</th>\n",
       "      <th>CARRIER.STATE</th>\n",
       "      <th>CARRIER.ZIP</th>\n",
       "      <th>my_dates</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week_type</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359138</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359145</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359146</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359141</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>COMPAGNIE MARITIME D-AFFRETEMENT</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EGLV143285966961</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>EVERGREEN LINE</td>\n",
       "      <td>NO 163 SEC 1 HSIN-NAN ROAD LUCHU HSIAN</td>\n",
       "      <td>TAOYUAN HSIEN 338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 SHIPPER SHIPPER.ADDRESS        CONSIGNEE CONSIGNEE.ADDRESS  \\\n",
       "0           2     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "1           4     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "2           5     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "3          10     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "4          11     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "\n",
       "  ZIPCODE NOTIFY NOTIFY.ADDRESS    BILL.OF.LADING ARRIVAL.DATE  ...  \\\n",
       "0       0    NaN            NaN     CMDUSZ2359138   09/20/2012  ...   \n",
       "1       0    NaN            NaN     CMDUSZ2359145   09/20/2012  ...   \n",
       "2       0    NaN            NaN     CMDUSZ2359146   09/20/2012  ...   \n",
       "3       0    NaN            NaN     CMDUSZ2359141   09/20/2012  ...   \n",
       "4       0    NaN            NaN  EGLV143285966961   09/20/2012  ...   \n",
       "\n",
       "                       CARRIER.NAME                         CARRIER.ADDRESS  \\\n",
       "0  COMPAGNIE MARITIME D-AFFRETEMENT                     5701 LAKE WRIGHT DR   \n",
       "1  COMPAGNIE MARITIME D-AFFRETEMENT                     5701 LAKE WRIGHT DR   \n",
       "2  COMPAGNIE MARITIME D-AFFRETEMENT                     5701 LAKE WRIGHT DR   \n",
       "3  COMPAGNIE MARITIME D-AFFRETEMENT                     5701 LAKE WRIGHT DR   \n",
       "4                    EVERGREEN LINE  NO 163 SEC 1 HSIN-NAN ROAD LUCHU HSIAN   \n",
       "\n",
       "        CARRIER.CITY  CARRIER.STATE CARRIER.ZIP   my_dates day_of_week  \\\n",
       "0            NORFOLK             VA       23502 2012-09-20    Thursday   \n",
       "1            NORFOLK             VA       23502 2012-09-20    Thursday   \n",
       "2            NORFOLK             VA       23502 2012-09-20    Thursday   \n",
       "3            NORFOLK             VA       23502 2012-09-20    Thursday   \n",
       "4  TAOYUAN HSIEN 338            NaN         NaN 2012-09-20    Thursday   \n",
       "\n",
       "  week_type      month  season  \n",
       "0   Weekday  September    Fall  \n",
       "1   Weekday  September    Fall  \n",
       "2   Weekday  September    Fall  \n",
       "3   Weekday  September    Fall  \n",
       "4   Weekday  September    Fall  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read in Validation Data for Prediction\n",
    "validation_df = pd.read_csv('/Users/jamessoley/Documents/altana/ds-project-validation.csv')\n",
    "\n",
    "### Weight KG -- Cleaned Name\n",
    "weight_kg_valid = validation_df['WEIGHT..KG.']\n",
    "\n",
    "### US.PORT\n",
    "### One Hot Encoding\n",
    "port_one_hot_valid = validation_df['US.PORT']\n",
    "port_dummies_valid = pd.get_dummies(port_one_hot_valid)\n",
    "print(port_one_hot_valid.drop_duplicates())\n",
    "\n",
    "### ARRIVAL.DATE\n",
    "### Create Variables to try and Capture the Driving Factors\n",
    "\n",
    "### Ensure that ARRIVAL.DATE is actually a datetime\n",
    "validation_df['my_dates'] = pd.to_datetime(validation_df['ARRIVAL.DATE'])\n",
    "\n",
    "### DOW\n",
    "validation_df['day_of_week'] = validation_df['my_dates'].dt.day_name()\n",
    "validation_df['week_type'] = validation_df.apply(weekend, axis=1)\n",
    "\n",
    "### Turn dow and week_type into Dummies\n",
    "dow_dummies_valid = pd.get_dummies(validation_df['day_of_week'])\n",
    "weektype_dummies_valid = pd.get_dummies(validation_df['week_type'])\n",
    "\n",
    "### Time of Year\n",
    "validation_df['month'] = validation_df['my_dates'].dt.month_name()\n",
    "validation_df['season'] = validation_df.apply(season, axis=1)\n",
    "\n",
    "### Turn dow and week_type into Dummies\n",
    "month_dummies_valid = pd.get_dummies(validation_df['month'])\n",
    "season_dummies_valid = pd.get_dummies(validation_df['season'])\n",
    "\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9775e",
   "metadata": {},
   "source": [
    "### Set up the Class Labels for y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ba07fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SHIPPER</th>\n",
       "      <th>SHIPPER.ADDRESS</th>\n",
       "      <th>CONSIGNEE</th>\n",
       "      <th>CONSIGNEE.ADDRESS</th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>NOTIFY</th>\n",
       "      <th>NOTIFY.ADDRESS</th>\n",
       "      <th>BILL.OF.LADING</th>\n",
       "      <th>ARRIVAL.DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>CARRIER.ADDRESS</th>\n",
       "      <th>CARRIER.CITY</th>\n",
       "      <th>CARRIER.STATE</th>\n",
       "      <th>CARRIER.ZIP</th>\n",
       "      <th>my_dates</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week_type</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>COUNTRY.OF.ORIGIN_TRIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359138</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359145</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359146</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CMDUSZ2359141</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>5701 LAKE WRIGHT DR</td>\n",
       "      <td>NORFOLK</td>\n",
       "      <td>VA</td>\n",
       "      <td>23502</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-NOT AVAILABLE-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EGLV143285966961</td>\n",
       "      <td>09/20/2012</td>\n",
       "      <td>...</td>\n",
       "      <td>NO 163 SEC 1 HSIN-NAN ROAD LUCHU HSIAN</td>\n",
       "      <td>TAOYUAN HSIEN 338</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-20</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>September</td>\n",
       "      <td>Fall</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 SHIPPER SHIPPER.ADDRESS        CONSIGNEE CONSIGNEE.ADDRESS  \\\n",
       "0           2     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "1           4     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "2           5     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "3          10     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "4          11     NaN             NaN  -NOT AVAILABLE-               NaN   \n",
       "\n",
       "  ZIPCODE NOTIFY NOTIFY.ADDRESS    BILL.OF.LADING ARRIVAL.DATE  ...  \\\n",
       "0       0    NaN            NaN     CMDUSZ2359138   09/20/2012  ...   \n",
       "1       0    NaN            NaN     CMDUSZ2359145   09/20/2012  ...   \n",
       "2       0    NaN            NaN     CMDUSZ2359146   09/20/2012  ...   \n",
       "3       0    NaN            NaN     CMDUSZ2359141   09/20/2012  ...   \n",
       "4       0    NaN            NaN  EGLV143285966961   09/20/2012  ...   \n",
       "\n",
       "                          CARRIER.ADDRESS       CARRIER.CITY CARRIER.STATE  \\\n",
       "0                     5701 LAKE WRIGHT DR            NORFOLK            VA   \n",
       "1                     5701 LAKE WRIGHT DR            NORFOLK            VA   \n",
       "2                     5701 LAKE WRIGHT DR            NORFOLK            VA   \n",
       "3                     5701 LAKE WRIGHT DR            NORFOLK            VA   \n",
       "4  NO 163 SEC 1 HSIN-NAN ROAD LUCHU HSIAN  TAOYUAN HSIEN 338           NaN   \n",
       "\n",
       "   CARRIER.ZIP   my_dates  day_of_week week_type      month season  \\\n",
       "0        23502 2012-09-20     Thursday   Weekday  September   Fall   \n",
       "1        23502 2012-09-20     Thursday   Weekday  September   Fall   \n",
       "2        23502 2012-09-20     Thursday   Weekday  September   Fall   \n",
       "3        23502 2012-09-20     Thursday   Weekday  September   Fall   \n",
       "4          NaN 2012-09-20     Thursday   Weekday  September   Fall   \n",
       "\n",
       "   COUNTRY.OF.ORIGIN_TRIM  \n",
       "0                   China  \n",
       "1                   China  \n",
       "2                   China  \n",
       "3                   China  \n",
       "4                   China  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Trimming Down the Classes to Less than or Equal to 10 Observations\n",
    "\n",
    "validation_df['COUNTRY.OF.ORIGIN_TRIM'] = train_df_labels.apply(country_other, axis=1)\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b01ba01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge in Class Labels from train_df_labels on 'COUNTRY.OF.ORIGIN_TRIM' from training_df to validation_df\n",
    "### Note: I know this is a bad way to do this but my pd.merge statements weren't working correctly (I'm sure I'm doing something wrong)\n",
    "\n",
    "def class_labeler(row):\n",
    "    if row['COUNTRY.OF.ORIGIN_TRIM'] == 'American Samoa':\n",
    "        return(0)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Argentina':\n",
    "        return(1)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Australia':\n",
    "        return(2)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Bahamas':\n",
    "        return(3)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Belgium':\n",
    "        return(4)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Brazil':\n",
    "        return(5)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Canada':\n",
    "        return(6)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Chile':\n",
    "        return(7)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'China':\n",
    "        return(8)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'China Taiwan':\n",
    "        return(9)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Colombia':\n",
    "        return(10)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Costa Rica':\n",
    "        return(11)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Dominican Republic':\n",
    "        return(12)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Egypt':\n",
    "        return(13)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'France':\n",
    "        return(14)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Germany':\n",
    "        return(15)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Greece':\n",
    "        return(16)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Guatemala':\n",
    "        return(17)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Haiti':\n",
    "        return(18)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Honduras':\n",
    "        return(19)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Hong Kong':\n",
    "        return(20)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'India':\n",
    "        return(21)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Israel':\n",
    "        return(22)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Italy':\n",
    "        return(23)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Jamaica':\n",
    "        return(24)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Japan':\n",
    "        return(25)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Jordan':\n",
    "        return(26)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Malaysia':\n",
    "        return(27)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Mexico':\n",
    "        return(28)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Netherlands':\n",
    "        return(29)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Oman':\n",
    "        return(30)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Other':\n",
    "        return(31)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Pakistan':\n",
    "        return(32)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Panama':\n",
    "        return(33)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Portugal':\n",
    "        return(34)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Romania':\n",
    "        return(35)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Singapore':\n",
    "        return(36)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'South Africa':\n",
    "        return(37)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'South Korea':\n",
    "        return(38)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Spain':\n",
    "        return(39)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Sri Lanka':\n",
    "        return(40)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Sweden':\n",
    "        return(41)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Thailand':\n",
    "        return(42)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Turkey':\n",
    "        return(43)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'United Arab Em':\n",
    "        return(44)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'United Kingdom':\n",
    "        return(45)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Unknown':\n",
    "        return(46)\n",
    "    elif row['COUNTRY.OF.ORIGIN_TRIM'] == 'Vietnam':\n",
    "        return(47)\n",
    "    else:\n",
    "        return(46)\n",
    "\n",
    "# Assign Class Labels\n",
    "validation_df['LABELS'] = validation_df.apply(class_labeler, axis=1)\n",
    "\n",
    "### Class Labels Need to Be Numeric Based on Updated Information _TRIM VAR\n",
    "validation_df['LABELS_FINAL'] = validation_df['LABELS'].astype('category')\n",
    "\n",
    "### Xs are Featured Engineered Above and y is Numeric Category Codes from Country \n",
    "y_valid = validation_df['LABELS_FINAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "158f7cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '00007', '00014', '00016', '00018', '00025', '00063', '0020', '0023', '0040', '0042', '0043', '0073', '0078742000008', '10', '100', '12', '13', '16', '18', '20', '2013', '24', '30', '40', '43', '48', '5oz', '9503', '9503000073', 'artificial', 'asst', 'black', 'canada', 'carrier', 'cartons', 'cbm', 'certify', 'chair', 'collect', 'com', 'consistent', 'copy', 'cotton', 'ctn', 'ctns', 'cy', 'declaration', 'declare', 'details', 'elwood', 'expeditors', 'fg', 'flow', 'footwear', 'freight', 'fresh', 'furniture', 'gln', 'houston', 'hs', 'hts', 'issue', 'itemized', 'kgs', 'knit', 'ladies', 'load', 'ms', 'nan', 'op', 'packing', 'pam', 'place', 'plastic', 'polyester', 'pono', 'regulated', 'release', 'sea', 'set', 'sk', 'solid', 'southgate', 'statesboro', 'steel', 'stock', 'storag', 'storage', 'style', 'suffolk', 'table', 'toys', 'value', 'vancouver', 'virtual', 'wal', 'way', 'white', 'wood']\n"
     ]
    }
   ],
   "source": [
    "### Expand the StopWords List by Eyeballing things that don't make sense\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['item','vendor','type','po','dept','description',\n",
    "                                            'pls','cargo','assortment','container','contains',\n",
    "                                            'material','materials','shipment','shipper','qty',\n",
    "                                            'purchase','product','procedures','pcs','order',\n",
    "                                            'destination','delivery','count','code','ct',\n",
    "                                            'department','negotiable','negotiated','non',\n",
    "                                            'number','packaging','supplier'])\n",
    "\n",
    "corpus = validation_df['PRODUCT.DETAILS'].values.astype('U')\n",
    "cv = CountVectorizer(max_features = 100, stop_words=stop_words)\n",
    "product_details_parsed = cv.fit_transform(corpus)\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f06c5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn Product Details into a DF and \n",
    "df_nlp_valid = pd.DataFrame(product_details_parsed.todense(), columns=cv.get_feature_names())\n",
    "\n",
    "### Update X to include the NLP Vectorized Features\n",
    "X1_valid = pd.concat([weight_kg_valid,port_dummies_valid,dow_dummies_valid,weektype_dummies_valid,month_dummies_valid,season_dummies_valid], axis=1).reset_index()\n",
    "X_valid = pd.concat([X1_valid,df_nlp_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fde4827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['18', 'Lawrence, Massachusetts',\n",
       "       'Los Angeles International Airport, Los Angeles, California',\n",
       "       'Newark, New Jersey', 'nan', 'statesboro', 'storag', 'table'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In X Valid\n",
    "X_valid.columns.difference(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d0291d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop Out Columns from X_valid not in X_train\n",
    "\n",
    "del X_valid['18']\n",
    "del X_valid['Lawrence, Massachusetts']\n",
    "del X_valid['Los Angeles International Airport, Los Angeles, California']\n",
    "del X_valid['Newark, New Jersey']\n",
    "del X_valid['nan']\n",
    "del X_valid['statesboro']\n",
    "del X_valid['storag']\n",
    "del X_valid['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7946744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2012', 'BOSTON', 'Brunswick, Georgia', 'CHARLESTON',\n",
       "       'Carquinez Strait, California', 'Fernandina Beach, Florida',\n",
       "       'Freeport, Texas', 'Houston Intercontinental Airport, Houston, Texas',\n",
       "       'Morgan City, Louisiana', 'New Haven, Connecticut',\n",
       "       'Newport News, Virginia', 'OAKLAND', 'OAKLAND, CA',\n",
       "       'Perth Amboy, New Jersey', 'Rochester, New York',\n",
       "       'Vancouver, Washington', 'allowed', 'bx', 'great', 'savannah'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In X_Train\n",
    "X_train.columns.difference(X_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f53e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Include Blank Columns where Columns in X_Train but not X_Valid\n",
    "\n",
    "X_valid['2012'] = 0\n",
    "X_valid['BOSTON'] = 0\n",
    "X_valid['Brunswick, Georgia'] = 0\n",
    "X_valid['CHARLESTON'] = 0\n",
    "X_valid['Carquinez Strait, California'] = 0\n",
    "X_valid['Fernandina Beach, Florida'] = 0\n",
    "X_valid['Freeport, Texas'] = 0\n",
    "X_valid['Houston Intercontinental Airport, Houston, Texas'] = 0\n",
    "X_valid['Morgan City, Louisiana'] = 0\n",
    "X_valid['New Haven, Connecticut'] = 0\n",
    "X_valid['Newport News, Virginia'] = 0\n",
    "X_valid['OAKLAND'] = 0\n",
    "X_valid['OAKLAND, CA'] = 0\n",
    "X_valid['Perth Amboy, New Jersey'] = 0\n",
    "X_valid['Rochester, New York'] = 0\n",
    "X_valid['Vancouver, Washington'] = 0\n",
    "X_valid['allowed'] = 0\n",
    "X_valid['bx'] = 0\n",
    "X_valid['great'] = 0\n",
    "X_valid['savannah'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2d282e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 49.93%\n"
     ]
    }
   ],
   "source": [
    "### Use Model 4 to Predict on X\n",
    "\n",
    "### Fit XGboost\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fit Model \n",
    "model = XGBClassifier() \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "### Predict on Validation Set (X_Valid) Using Model 4 Training\n",
    "y_pred = model.predict(X_valid) \n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_valid, predictions) \n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e623493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Argentina',\n",
       " 'Australia',\n",
       " 'Bahamas',\n",
       " 'Belgium',\n",
       " 'Brazil',\n",
       " 'Canada',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'China Taiwan',\n",
       " 'Colombia',\n",
       " 'Costa Rica',\n",
       " 'Dominican Republic',\n",
       " 'Egypt',\n",
       " 'France',\n",
       " 'Germany',\n",
       " 'Guatemala',\n",
       " 'Honduras',\n",
       " 'Hong Kong',\n",
       " 'India',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Jamaica',\n",
       " 'Japan',\n",
       " 'Jordan',\n",
       " 'Malaysia',\n",
       " 'Mexico',\n",
       " 'Netherlands',\n",
       " 'Oman',\n",
       " 'Other',\n",
       " 'Pakistan',\n",
       " 'Panama',\n",
       " 'Portugal',\n",
       " 'Singapore',\n",
       " 'South Africa',\n",
       " 'South Korea',\n",
       " 'Spain',\n",
       " 'Sri Lanka',\n",
       " 'Sweden',\n",
       " 'Thailand',\n",
       " 'Turkey',\n",
       " 'United Arab Em',\n",
       " 'United Kingdom',\n",
       " nan,\n",
       " 'Unknown',\n",
       " 'Vietnam']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42fb1e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00        27\n",
      "           4       0.00      0.00      0.00        22\n",
      "           5       0.00      0.00      0.00        18\n",
      "           6       0.00      0.00      0.00        62\n",
      "           7       0.00      0.00      0.00       648\n",
      "           8       0.75      0.66      0.70     34416\n",
      "           9       0.01      0.01      0.01       630\n",
      "          10       0.00      0.00      0.00        26\n",
      "          11       0.00      0.00      0.00       745\n",
      "          12       0.00      0.02      0.00        49\n",
      "          13       0.92      0.57      0.70       254\n",
      "          14       0.00      0.00      0.00         9\n",
      "          15       0.00      0.00      0.00        11\n",
      "          17       0.01      0.00      0.00      1108\n",
      "          19       0.00      0.00      0.00       145\n",
      "          20       0.18      0.25      0.21      1853\n",
      "          21       0.02      0.02      0.02       326\n",
      "          22       0.00      0.00      0.00         2\n",
      "          23       0.00      0.00      0.00        62\n",
      "          24       0.00      0.00      0.00        58\n",
      "          25       0.05      0.36      0.09        25\n",
      "          26       0.00      0.00      0.00        23\n",
      "          27       0.02      0.01      0.01       768\n",
      "          28       0.00      0.00      0.00        13\n",
      "          29       0.00      0.00      0.00        12\n",
      "          30       0.09      0.00      0.00       870\n",
      "          31       0.00      0.00      0.00        23\n",
      "          32       0.02      0.00      0.01       279\n",
      "          33       0.03      0.10      0.04      1242\n",
      "          34       0.00      0.00      0.00        21\n",
      "          36       0.09      0.09      0.09      1213\n",
      "          37       0.00      0.00      0.00        54\n",
      "          38       0.05      0.17      0.08      1284\n",
      "          39       0.00      0.00      0.00       169\n",
      "          40       0.00      0.00      0.00       426\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.00      0.00      0.00       240\n",
      "          43       0.00      0.00      0.00         2\n",
      "          44       0.00      0.00      0.00         1\n",
      "          45       0.00      0.00      0.00         8\n",
      "          46       0.00      0.00      0.00       416\n",
      "          47       0.00      0.00      0.00       192\n",
      "\n",
      "    accuracy                           0.50     47760\n",
      "   macro avg       0.05      0.05      0.04     47760\n",
      "weighted avg       0.56      0.50      0.52     47760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jamessoley/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Get the Labels in the Report\n",
    "df1 = validation_df.sort_values('LABELS_FINAL', ascending=True).drop_duplicates('COUNTRY.OF.ORIGIN_TRIM').sort_index()\n",
    "country_key = df1[['LABELS_FINAL','COUNTRY.OF.ORIGIN_TRIM']].sort_values('LABELS_FINAL', ascending=True)\n",
    "country_key_values = country_key['COUNTRY.OF.ORIGIN_TRIM'].tolist()\n",
    "\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8508e51",
   "metadata": {},
   "source": [
    "#### Something clearly isn't quite right here. My model which performed very well on the 1/3rd holdout .92% accuracy in Model 4 isn't performing well at all on the validation set. A number of things could have gone wrong: 1) I may have messed up a class label strategy but I checked that a few times 2) I may have severly overfit the data but my understanding is that Xgboost does alot of work under the hood to protect against this (L1 & L2 regularization, max depth, num trees) so I'll be surprised if this is happening 3) The Validation set is very different than the training set which is always something that can be a problem and I need to think about how to reduce the error that is presented here. I've hit the 3+ hours at this point so I'll stop and leave it as a matter of discussion for the interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddaf3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1bbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b013d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6aaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bbee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
